{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n",
      "loading is over.\n",
      "over.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(os.getcwd())\n",
    "code_path = '/root/ieee/pipeline/'\n",
    "sys.path.append(code_path)\n",
    "\n",
    "\n",
    "path = '/root/ieee/'\n",
    "\n",
    "train_transaction = pd.read_csv(path + 'train_transaction.csv')\n",
    "test_transaction = pd.read_csv(path + 'test_transaction.csv')\n",
    "\n",
    "train_identity = pd.read_csv(path + 'train_identity.csv')\n",
    "test_identity = pd.read_csv(path + 'test_identity.csv')\n",
    "print(\"loading is over.\")\n",
    "train_transaction.sort_values('TransactionDT', inplace = True)\n",
    "test_transaction.sort_values('TransactionDT', inplace = True)\n",
    "train_transaction['nulls1'] = train_transaction.isna().sum(axis=1)\n",
    "test_transaction['nulls1'] = test_transaction.isna().sum(axis=1)\n",
    "\n",
    "card_feature = [col for col in train_transaction.columns if \"card\" in col] #category\n",
    "addr_feature = [col for col in train_transaction.columns if \"addr\" in col] #category\n",
    "dist_feature = [col for col in train_transaction.columns if \"dist\" in col] #numeric\n",
    "mail_feature = [col for col in train_transaction.columns if \"email\" in col] #category\n",
    "\n",
    "c_feature = [col for col in train_transaction.columns if \"C\" in col] #numeric\n",
    "#C1-C14: counting, \n",
    "#    such as how many addresses are found to be associated with the payment card, etc. \n",
    "#The actual meaning is masked.\n",
    "d_feature = [col for col in train_transaction.columns if \"D\" in col] #numeric \n",
    "#D1-D15: timedelta, such as days between previous transaction, etc.\n",
    "d_feature.remove('TransactionID')\n",
    "d_feature.remove('TransactionDT')\n",
    "d_feature.remove('ProductCD')\n",
    "c_feature.remove('ProductCD')\n",
    "m_feature = [col for col in train_transaction.columns if \"M\" in col] #category\n",
    "v_feature = [col for col in train_transaction.columns if \"V\" in col] #numeric\n",
    "\n",
    "train_transaction = train_transaction.loc[train_transaction.card6 != 'debit or credit'].reset_index(drop = True)\n",
    "\n",
    "C1_threshold = test_transaction.C1.mean() + test_transaction.C1.std() * 2.5\n",
    "train_transaction = train_transaction[train_transaction.C1<C1_threshold].reset_index(drop = True)\n",
    "\n",
    "# col = 'D1'\n",
    "train_transaction['Transaction_day'] = np.floor((train_transaction['TransactionDT'] / (3600 * 24) - 1))\n",
    "test_transaction['Transaction_day'] = np.floor((test_transaction['TransactionDT'] / (3600 * 24) - 1))\n",
    "c_feat = [col for col in c_feature if col not in ['C13']]\n",
    "# c_feat = [col for col in c_feature if col not in ['C13', 'C14']] #modify 2019.09.17 out of EDA\n",
    "\n",
    "train_transaction['linear'] = train_transaction['Transaction_day']\n",
    "test_transaction['linear'] = test_transaction['Transaction_day']\n",
    "train_transaction['D4_new'] = train_transaction['D4']/test_transaction['linear']\n",
    "test_transaction['D4_new'] = test_transaction['D4']/test_transaction['linear']\n",
    "print(\"over.\")\n",
    "\n",
    "best_feature = pd.read_csv(\"/root/ieee/best_feat_v3.csv\")\n",
    "best_feature = list(best_feature.loc[best_feature.gain_score >= 0, 'feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.mail related feature: - done in 5s\n",
      "2.numeric type encoding feature: - done in 0s\n",
      "3.identity flag count encoding: - done in 7s\n",
      "4.address type feature: - done in 1s\n",
      "5.product type feature: - done in 0s\n",
      "6.match type feature: - done in 2s\n",
      "7.card type feature: - done in 1s\n",
      "8.category type feature: - done in 6s\n",
      "9.numeric feature: - done in 10s\n",
      "10. C count feature: - done in 1s\n",
      "12. time related feature: - done in 2s\n",
      "13. D related feature: - done in 1s\n",
      "over.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from feature import woe_encoder, timer, mail_func, addr_func, \\\n",
    "numeric_func, identity_func, \\\n",
    "product_func, match_func,card_func, \\\n",
    "all_category_encoding, Transaction_amt_encoding, \\\n",
    "C_feature, pca_missing, date_feature, D_feature\n",
    "\n",
    "\n",
    "def feature_engineering(train_transaction, test_transaction):\n",
    "    basic_feature = []\n",
    "    data = pd.DataFrame([])\n",
    "    tr_shape = train_transaction.shape[0]\n",
    "    if mail_flag:\n",
    "        with timer(\"1.mail related feature:\"):\n",
    "            temp = mail_func(train_transaction, test_transaction)\n",
    "            data[temp.columns] = temp\n",
    "            del temp;gc.collect()\n",
    "    if num_flag:\n",
    "        with timer(\"2.numeric type encoding feature:\"):\n",
    "            temp = numeric_func(train_transaction, test_transaction)\n",
    "            data[temp.columns] = temp\n",
    "            del temp;gc.collect()\n",
    "    if iden_flag:\n",
    "        with timer(\"3.identity flag count encoding:\"):\n",
    "            temp = identity_func(train_transaction, test_transaction, train_identity, test_identity)\n",
    "            data[temp.columns] = temp\n",
    "            del temp;gc.collect()\n",
    "    if addr_flag:\n",
    "        with timer(\"4.address type feature:\"):\n",
    "            temp = addr_func(train_transaction, test_transaction)\n",
    "            data[temp.columns] = temp\n",
    "            del temp;gc.collect()\n",
    "    if prod_flag:\n",
    "        with timer(\"5.product type feature:\"):\n",
    "            temp = product_func(train_transaction, test_transaction)\n",
    "            data[temp.columns] = temp\n",
    "            del temp;gc.collect()\n",
    "    if match_flag:\n",
    "        with timer(\"6.match type feature:\"):\n",
    "            temp = match_func(train_transaction, test_transaction)\n",
    "            data[temp.columns] = temp\n",
    "            del temp;gc.collect()      \n",
    "    if card_flag:\n",
    "        with timer(\"7.card type feature:\"):\n",
    "            temp = card_func(train_transaction, test_transaction)\n",
    "            data[temp.columns] = temp\n",
    "            del temp;gc.collect()      \n",
    "    if category_flag:\n",
    "        with timer(\"8.category type feature:\"):\n",
    "            temp = all_category_encoding(train_transaction, test_transaction)\n",
    "            data[temp.columns] = temp\n",
    "            del temp;gc.collect()\n",
    "    if numeric_flag:\n",
    "        with timer(\"9.numeric feature:\"):\n",
    "            temp = Transaction_amt_encoding(train_transaction, test_transaction)\n",
    "            data[temp.columns] = temp\n",
    "            del temp;gc.collect()\n",
    "    if C_flag:\n",
    "        with timer(\"10. C count feature:\"):\n",
    "            temp = C_feature(train_transaction, test_transaction)\n",
    "            data[temp.columns] = temp\n",
    "            del temp;gc.collect()\n",
    "    if miss_flag:\n",
    "        with timer(\"11. missing pca feature:\"):\n",
    "            temp = pca_missing(train_transaction, test_transaction)\n",
    "            data[temp.columns] = temp\n",
    "            del temp;gc.collect()\n",
    "    if date_flag:\n",
    "        with timer(\"12. time related feature:\"):\n",
    "            temp = date_feature(train_transaction, test_transaction)\n",
    "            data[temp.columns] = temp\n",
    "            del temp;gc.collect()\n",
    "    if D_flag:\n",
    "        with timer(\"13. D related feature:\"):\n",
    "            temp = D_feature(train_transaction, test_transaction)\n",
    "            data[temp.columns] = temp\n",
    "            del temp;gc.collect()\n",
    "    if label_flag:\n",
    "        with timer(\"14. Label encoding:\"):\n",
    "            temp = label_encoding(train_transaction, test_transaction)\n",
    "            data[temp.columns] = temp\n",
    "            del temp;gc.collect()\n",
    "    if wise_flag:\n",
    "        with timer(\"15. card wise anlaysis:\"):\n",
    "            temp = card_wise_exp(train_transaction, test_transaction)\n",
    "            data[temp.columns] = temp\n",
    "            del temp;gc.collect()\n",
    "    if Dcount_flag:\n",
    "        with timer(\"16. day_related_counting:\"):\n",
    "            temp = day_related_counting(train_transaction, test_transaction)\n",
    "            data[temp.columns] = temp\n",
    "            del temp;gc.collect()\n",
    "        \n",
    "    train_transaction[data.columns] = data[:tr_shape].reset_index(drop = True)\n",
    "    test_transaction[data.columns] = data[tr_shape:].reset_index(drop = True)\n",
    "    print(\"over.\")\n",
    "    return train_transaction, test_transaction, list(data.columns)\n",
    "\n",
    "\n",
    "mail_flag = True\n",
    "num_flag = True\n",
    "iden_flag = True\n",
    "addr_flag = True\n",
    "prod_flag = True\n",
    "\n",
    "match_flag = True\n",
    "card_flag = True\n",
    "\n",
    "category_flag = True\n",
    "numeric_flag = True\n",
    "\n",
    "C_flag = True\n",
    "miss_flag = False\n",
    "date_flag = True\n",
    "D_flag = True\n",
    "label_flag = False\n",
    "wise_flag = False\n",
    "Dcount_flag = False\n",
    "\n",
    "train_transaction, test_transaction, basic_feature = feature_engineering(train_transaction, test_transaction)\n",
    "features = c_feature + d_feature + dist_feature + v_feature + ['TransactionAmt','nulls1']\n",
    "gc.collect()\n",
    "\n",
    "del train_identity, test_identity;gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(587087, 542) (506691, 541)\n",
      "diff:205.52618670463562.s\n",
      "diff:30.166819095611572.s\n",
      "diff:398.79793190956116.s\n",
      "(587087, 681) (506691, 680)\n",
      "over.\n",
      "CPU times: user 10min 28s, sys: 26.2 s, total: 10min 54s\n",
      "Wall time: 10min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def safe_div(a, b):\n",
    "    try:\n",
    "        return float(a) / float(b)\n",
    "    except:\n",
    "        return 0.0\n",
    "def time_diff(tr, ts):\n",
    "#     H_move = 12\n",
    "    H_move = 0\n",
    "    v_new = ['V{}'.format(i) for i in range(306, 312)]\n",
    "    card_feature = [col for col in tr.columns if \"card\" in col] #category\n",
    "    addr_feature = [col for col in tr.columns if \"addr\" in col] #category\n",
    "    feat = ['TransactionID',\"P_emaildomain\", 'TransactionDT', 'TransactionAmt'] +\\\n",
    "            d_feature + card_feature + addr_feature + c_feature + v_new + dist_feature\n",
    "    tr_ = tr[feat]\n",
    "    ts_ = ts[feat]\n",
    "    data_train = pd.DataFrame([])\n",
    "    data_test = pd.DataFrame([])\n",
    "    t1 = time.time()\n",
    "    \n",
    "    tr_['cid'] = \\\n",
    "    tr[c_feat + card_feature].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    ts_['cid'] = \\\n",
    "    ts[c_feat + card_feature].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "    tr_['uid'] = \\\n",
    "    tr[addr_feature + card_feature + [\"P_emaildomain\"]].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    ts_['uid'] = \\\n",
    "    ts[addr_feature + card_feature + [\"P_emaildomain\"]].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "#     tr_['uid'] = tr['card1'].astype(str) + \"_\" + tr['card2'].astype(str) +\\\n",
    "#                         tr['card3'].astype(str) + \"_\" + tr['card4'].astype(str) +\\\n",
    "#                         tr['card5'].astype(str) + \"_\" + tr['card6'].astype(str) +\\\n",
    "#                         tr['addr1'].astype(str) + \"_\" + tr['addr2'].astype(str) +\\\n",
    "#                         \"_\" + tr['P_emaildomain'].astype(str)\n",
    "#     ts_['uid'] = ts['card1'].astype(str) + \"_\" + ts['card2'].astype(str) +\\\n",
    "#                         ts['card3'].astype(str) + \"_\" + ts['card4'].astype(str) +\\\n",
    "#                         ts['card5'].astype(str) + \"_\" + ts['card6'].astype(str) +\\\n",
    "#                         ts['addr1'].astype(str) + \"_\" + ts['addr2'].astype(str) +\\\n",
    "#                         \"_\" + ts['P_emaildomain'].astype(str)\n",
    "    tr_[\"day\"] = (tr[\"TransactionDT\"] + 3600 * H_move) // (24 * 60 * 60)\n",
    "    ts_[\"day\"] = (ts[\"TransactionDT\"] + 3600 * H_move) // (24 * 60 * 60)\n",
    "    tr_['Hour'] = np.floor(tr['TransactionDT'] / 3600) % 24\n",
    "    ts_['Hour'] = np.floor(ts['TransactionDT'] / 3600) % 24\n",
    "    tr_['D1_delta'] = tr_['D1'] - tr_['day']\n",
    "    ts_['D1_delta'] = ts_['D1'] - ts_['day']\n",
    "    tr_['D2_delta'] = tr_['D2'] - tr_['day']\n",
    "    ts_['D2_delta'] = ts_['D2'] - ts_['day']\n",
    "    tr_['D10_delta'] = tr_['D10'] - tr_['day']\n",
    "    ts_['D10_delta'] = ts_['D10'] - ts_['day']\n",
    "    tr_['D15_delta'] = tr_['D15'] - tr_['day']\n",
    "    ts_['D15_delta'] = ts_['D15'] - ts_['day']\n",
    "\n",
    "    tr_['D1_delta_uid'] = tr_['D1_delta'].astype(str) + \"_\" + tr_['uid']\n",
    "    tr_['D2_delta_uid'] = tr_['D2_delta'].astype(str) + \"_\" + tr_['uid']\n",
    "    ts_['D1_delta_uid'] = ts_['D1_delta'].astype(str) + \"_\" + ts_['uid']\n",
    "    ts_['D2_delta_uid'] = ts_['D2_delta'].astype(str) + \"_\" + ts_['uid']\n",
    "\n",
    "    tr_['D10_delta_uid'] = tr_['D10_delta'].astype(str) + \"_\" + tr_['uid']\n",
    "    tr_['D15_delta_uid'] = tr_['D15_delta'].astype(str) + \"_\" + tr_['uid']\n",
    "    ts_['D10_delta_uid'] = ts_['D10_delta'].astype(str) + \"_\" + ts_['uid']\n",
    "    ts_['D15_delta_uid'] = ts_['D15_delta'].astype(str) + \"_\" + ts_['uid']\n",
    "    \n",
    "    tr_['c4_d1_id'] = \\\n",
    "    tr_[['C4', 'D1_delta'] + card_feature].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    ts_['c4_d1_id'] = \\\n",
    "    ts_[['C4', 'D1_delta'] + card_feature].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    \n",
    "#     tr_['cd_uid'] = \\\n",
    "#     tr_[['D1_delta_uid'] + c_feat].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "#     ts_['cd_uid'] = \\\n",
    "#     ts_[['D1_delta_uid'] + c_feat].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "    tr_['cd_uid'] = \\\n",
    "    tr_[['D1_delta_uid'] + ['C1', 'C4']].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    ts_['cd_uid'] = \\\n",
    "    ts_[['D1_delta_uid'] + ['C1', 'C4']].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "    t2 = time.time()\n",
    "    print(\"diff:{}.s\".format(t2 - t1))\n",
    "    # TransactionAmt相关\n",
    "    # 当天的交易次数\n",
    "    data_train['trans_curday_Amt_cnt'] = tr_.groupby(['uid', 'day'])['TransactionAmt'].transform('count')\n",
    "    data_test['trans_curday_Amt_cnt'] = ts_.groupby(['uid', 'day'])['TransactionAmt'].transform('count')\n",
    "    # 当天的交易总额\n",
    "    data_train['trans_curday_Amt_sum'] = tr_.groupby(['uid', 'day'])['TransactionAmt'].transform('sum')\n",
    "    data_test['trans_curday_Amt_sum'] = ts_.groupby(['uid', 'day'])['TransactionAmt'].transform('sum')\n",
    "    # 当天的交易最大金额\n",
    "    data_train['trans_curday_Amt_max'] = tr_.groupby(['uid', 'day'])['TransactionAmt'].transform('max')\n",
    "    data_test['trans_curday_Amt_max'] = ts_.groupby(['uid', 'day'])['TransactionAmt'].transform('max')\n",
    "    # 当天的交易最小金额\n",
    "    data_train['trans_curday_Amt_min'] = tr_.groupby(['uid', 'day'])['TransactionAmt'].transform('min')\n",
    "    data_test['trans_curday_Amt_min'] = ts_.groupby(['uid', 'day'])['TransactionAmt'].transform('min')\n",
    "    # 当天的交易平均金额\n",
    "    data_train['trans_curday_Amt_mean'] = tr_.groupby(['uid', 'day'])['TransactionAmt'].transform('mean')\n",
    "    data_test['trans_curday_Amt_mean'] = ts_.groupby(['uid', 'day'])['TransactionAmt'].transform('mean')\n",
    "    # 当天同样金额的交易次数\n",
    "    data_train['trans_curday_samAmt_cnt'] = tr_.groupby(['uid', 'day', 'TransactionAmt'])['TransactionAmt'].transform('count')\n",
    "    data_test['trans_curday_samAmt_cnt'] = ts_.groupby(['uid', 'day', 'TransactionAmt'])['TransactionAmt'].transform('count')\n",
    "\n",
    "    data_train['trans_curday_hour_max'] = tr_.groupby(['uid', 'day'])['Hour'].transform('max')\n",
    "    data_test['trans_curday_hour_max'] = ts_.groupby(['uid', 'day'])['Hour'].transform('max')\n",
    "    # 当天的交易最小Hour\n",
    "    data_train['trans_curday_hour_min'] = tr_.groupby(['uid', 'day'])['Hour'].transform('min')\n",
    "    data_test['trans_curday_hour_min'] = ts_.groupby(['uid', 'day'])['Hour'].transform('min')\n",
    "    # 当天的交易平均Hour\n",
    "    data_train['trans_curday_hour_mean'] = tr_.groupby(['uid', 'day'])['Hour'].transform('mean')\n",
    "    data_test['trans_curday_hour_mean'] = ts_.groupby(['uid', 'day'])['Hour'].transform('mean')\n",
    "    t3 = time.time()\n",
    "    print(\"diff:{}.s\".format(t3 - t2))\n",
    "    \n",
    "    \n",
    "#     #当天同样金额的交易次数/全部\n",
    "#     data_train['trans_curday_ratio'] = data_train['trans_curday_samAmt_cnt']/data_train['trans_curday_Amt_cnt']\n",
    "#     data_test['trans_curday_ratio'] = data_test['trans_curday_samAmt_cnt']/data_test['trans_curday_Amt_cnt']\n",
    "#     # 当天交易金额/最大金额\n",
    "#     data_train['trans_Amt_ratio'] = tr_['TransactionAmt']/data_train['trans_curday_Amt_max']\n",
    "#     data_test['trans_Amt_ratio'] = ts_['TransactionAmt']/data_test['trans_curday_Amt_max']\n",
    "#     # 最小值/最大值\n",
    "#     data_train['trans_Amt_max_min_ratio'] = data_train['trans_curday_Amt_min']/data_train['trans_curday_Amt_max']\n",
    "#     data_test['trans_Amt_max_min_ratio'] = data_test['trans_curday_Amt_min']/data_test['trans_curday_Amt_max']\n",
    "    \n",
    "    data = pd.concat([data_train, data_test]).reset_index(drop = True)\n",
    "    del data_train, data_test;gc.collect()\n",
    "    # 距离上一笔以及下一笔交易的时间差特征(seconds)\n",
    "    key_list = ['uid', 'D1_delta_uid', 'D2_delta_uid', 'c4_d1_id', 'cid']\n",
    "#     values = ['TransactionDT', 'TransactionAmt'] + c_feature\n",
    "    values = ['TransactionDT', 'TransactionAmt', 'C13', 'day']\n",
    "#     values = ['TransactionDT', 'TransactionAmt', 'C13']\n",
    "    df = pd.concat([tr_, ts_]).reset_index(drop = True)\n",
    "    for key in key_list:\n",
    "        for value in values:\n",
    "            stat_temp = df[[key] + [value]].copy()\n",
    "            for i in [-2, 2, -1, 1]:\n",
    "                shift_value = stat_temp.groupby(key)[value].shift(i)\n",
    "                cname = '_'.join([key, value]) + '_diff_time{}'.format(i)\n",
    "                data[cname] = stat_temp[value] - shift_value\n",
    "#                 cname = '_'.join([key, value]) + '_div_time{}'.format(i)\n",
    "#                 data[cname] = stat_temp[value]/(0.01 + shift_value)\n",
    "\n",
    "#     key_list = ['uid', 'D1_delta_uid', 'c4_d1_id', 'cid']    \n",
    "    key_list = ['uid', 'D1_delta_uid', 'D2_delta_uid', 'c4_d1_id', 'cid'] #update from 19.09.16 20:52\n",
    "#     stat_temp = df[key_list + d_feature+ ['TransactionAmt', 'C13'] + v_new].copy()\n",
    "    for key in key_list:\n",
    "        for col in ['D1', 'D2', 'D10', 'D15'] + ['TransactionAmt']:\n",
    "            stat_temp = df[[key] + [col]].copy()\n",
    "            for i in [-1, 1]:\n",
    "                shift_value = stat_temp.groupby(key)[col].shift(i)\n",
    "                cname = '_'.join([key, col]) +'_shift_time{}'.format(i)\n",
    "                data[cname] = shift_value\n",
    "\n",
    "    t4 = time.time()\n",
    "    print(\"diff:{}.s\".format(t4 - t3))\n",
    "    \n",
    "    return data\n",
    "\n",
    "print(train_transaction.shape, test_transaction.shape)\n",
    "tr_shape = train_transaction.shape[0]\n",
    "new_feat = time_diff(train_transaction, test_transaction)\n",
    "train_transaction[new_feat.columns] = new_feat[:tr_shape].reset_index(drop = True)\n",
    "test_transaction[new_feat.columns] = new_feat[tr_shape:].reset_index(drop = True)\n",
    "new_cols = list(new_feat.columns)\n",
    "print(train_transaction.shape, test_transaction.shape)\n",
    "del new_feat;gc.collect()\n",
    "print(\"over.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(587087, 681) (506691, 680)\n",
      "FE_MIN_(TransactionDT)_(C1)\n",
      "FE_MIN_(card2)_(C1)\n",
      "FE_MIN_(card1)_(C1)\n",
      "FE_MIN_(addr1)_(C1)\n",
      "FE_MIN_(card1)_(C13)\n",
      "FE_MIN_(card1)_(C14)\n",
      "FE_MIN_(card5)_(C1)\n",
      "FE_MAX_(TransactionDT)_(C1)\n",
      "FE_MAX_(card2)_(C1)\n",
      "FE_MAX_(card1)_(C1)\n",
      "FE_MAX_(addr1)_(C1)\n",
      "FE_MAX_(TransactionDT)_(C13)\n",
      "FE_MAX_(P_emaildomain)_(C1)\n",
      "FE_MAX_(card2)_(C13)\n",
      "FE_MAX_(card1)_(C13)\n",
      "FE_MAX_(TransactionDT)_(C14)\n",
      "FE_MAX_(TransactionDT)_(C12)\n",
      "FE_MAX_(addr1)_(C13)\n",
      "FE_MAX_(card2)_(C14)\n",
      "FE_MAX_(card1)_(C14)\n",
      "FE_MAX_(TransactionDT)_(TransactionAmt)\n",
      "FE_MAX_(card5)_(C1)\n",
      "(587087, 703) (506691, 702)\n",
      "CPU times: user 41.7 s, sys: 1min 50s, total: 2min 32s\n",
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def FE_OP_c1_c2(df, op = 'count', df_test = None):\n",
    "    op = op.lower()\n",
    "    c1, c2 = df.columns.tolist()\n",
    "    c1c2 = 'FE_{}_({})_({})'.format(op.upper(), c1, c2)\n",
    "    \n",
    "    if df_test is not None:\n",
    "        n_train = len(df)\n",
    "        df = pd.concat([df, df_test], axis = 0).reset_index(drop = True)\n",
    "\n",
    "    if op == 'count':\n",
    "        s = df.groupby([c1, c2])[c1].transform(op)\n",
    "    elif op in ['nunique', 'median', 'mean', 'min', 'max', 'std', 'sum', 'cumcount']:\n",
    "        s = df.groupby(c1)[c2].transform(op)\n",
    "    elif op == 'add':\n",
    "        s = df[c1] + df[c2]\n",
    "    elif op == 'diff':\n",
    "        s = df[c1] - df[c2]\n",
    "    elif op == 'mul':\n",
    "        s = df[c1] * df[c2]\n",
    "    elif op == 'div':\n",
    "        s = df[c1] / df[c2]\n",
    "    \n",
    "    if df_test is not None:\n",
    "        return c1c2, s[:n_train].reset_index(drop = True), s[n_train:].reset_index(drop = True)\n",
    "    return c1c2, s\n",
    "\n",
    "def decompose_name(col):\n",
    "    c1, c2 = col.split(')_(')\n",
    "    op, c1 = c1.split('_(')\n",
    "    op = op[3:].lower()\n",
    "    c2 = c2[:-1]\n",
    "    return op, c1, c2\n",
    "\n",
    "def name_to_fe(train, test, cols_add):\n",
    "    for col in cols_add:\n",
    "        print(col)\n",
    "        op, c1, c2 = decompose_name(col)\n",
    "        c1c2, train[col], test[col] =  FE_OP_c1_c2(train[[c1,c2]], op, test[[c1,c2]])\n",
    "    return train, test\n",
    "\n",
    "cols_add = [\n",
    "'FE_MIN_(TransactionDT)_(C1)',\n",
    " 'FE_MIN_(card2)_(C1)',\n",
    " 'FE_MIN_(card1)_(C1)',\n",
    " 'FE_MIN_(addr1)_(C1)',\n",
    " 'FE_MIN_(card1)_(C13)',\n",
    " 'FE_MIN_(card1)_(C14)',\n",
    " 'FE_MIN_(card5)_(C1)',\n",
    " 'FE_MAX_(TransactionDT)_(C1)',\n",
    " 'FE_MAX_(card2)_(C1)',\n",
    " 'FE_MAX_(card1)_(C1)',\n",
    " 'FE_MAX_(addr1)_(C1)',\n",
    " 'FE_MAX_(TransactionDT)_(C13)',\n",
    " 'FE_MAX_(P_emaildomain)_(C1)',\n",
    " 'FE_MAX_(card2)_(C13)',\n",
    " 'FE_MAX_(card1)_(C13)',\n",
    " 'FE_MAX_(TransactionDT)_(C14)',\n",
    " 'FE_MAX_(TransactionDT)_(C12)',\n",
    " 'FE_MAX_(addr1)_(C13)',\n",
    " 'FE_MAX_(card2)_(C14)',\n",
    " 'FE_MAX_(card1)_(C14)',\n",
    " 'FE_MAX_(TransactionDT)_(TransactionAmt)',\n",
    " 'FE_MAX_(card5)_(C1)']\n",
    "\n",
    "print(train_transaction.shape, test_transaction.shape)\n",
    "original_cols = list(train_transaction.columns)\n",
    "train_transaction, test_transaction = name_to_fe(train_transaction, test_transaction, cols_add)\n",
    "new_cols1 = [col for col in train_transaction.columns if col not in original_cols]\n",
    "print(train_transaction.shape, test_transaction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(587087, 720) (506691, 724)\n",
      "(587087, 774) (506691, 778)\n",
      "over.\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "def amt_accumulation_summary(tr, ts):\n",
    "    v_list = ['V47', 'V244', 'V246', 'V257']\n",
    "    key = card_feature + addr_feature + c_feature + d_feature + v_list\n",
    "    df = pd.concat([tr[key + ['Transaction_day', 'TransactionDT','TransactionAmt','P_emaildomain']], \n",
    "                    ts[key + ['Transaction_day', 'TransactionDT','TransactionAmt','P_emaildomain']]]).reset_index(drop = True)\n",
    "    H_move = 0\n",
    "    df[\"Transaction_day\"] = (df[\"TransactionDT\"] + 3600 * H_move) // (24 * 60 * 60)\n",
    "    col = 'D1'\n",
    "    df[col + '_new'] = df[col] - df['Transaction_day']\n",
    "    col = 'D2'\n",
    "    df[col + '_new'] = df[col] - df['Transaction_day']\n",
    "    data = pd.DataFrame([])\n",
    "#     df['uid'] = df[card_feature + addr_feature].apply(\n",
    "#                     lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "    df['uid'] = df[card_feature + addr_feature + [\"P_emaildomain\"]].apply(\n",
    "                    lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    df['d1_uid'] = df['uid'].astype(str) + \"_\" + df['D1_new'].astype(str)\n",
    "    df['d2_uid'] = df['uid'].astype(str) + \"_\" + df['D2_new'].astype(str)\n",
    "    \n",
    "    df['c4_d1_id'] = \\\n",
    "    df[['C4', 'D1_new'] + card_feature].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    df['c14_d1_uid'] = \\\n",
    "    df[['C14', 'D1_new']+ card_feature].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    used = []\n",
    "    for key in ['uid', 'd1_uid', 'c4_d1_id']:\n",
    "        for col in ['TransactionAmt'] + ['D1']:\n",
    "            df['{}_diff_1_{}'.format(col, key)] = \\\n",
    "            df[col] - df.groupby(key)[col].shift(1)\n",
    "            df['{}_diff_-1_{}'.format(col, key)] = \\\n",
    "            df[col] - df.groupby(key)[col].shift(-1)\n",
    "            used.append(('{}_diff_1_{}'.format(col, key), key))\n",
    "            used.append(('{}_diff_-1_{}'.format(col, key), key))\n",
    "    for func in ['mean', 'std', 'median']: #19.09.22\n",
    "        for col in used:\n",
    "            data[\"_\".join([col[0], func])] = df.groupby(col[1])[col[0]].transform(func)\n",
    "\n",
    "            \n",
    "    new_used = []\n",
    "    for key in ['uid', 'd1_uid', 'c4_d1_id']:\n",
    "#         ['C1', 'C13', 'C14'] + ['D1_new']\n",
    "        for col in ['C1', 'C14']:\n",
    "            df['{}_encoding_{}'.format(col, key)] = df[col]\n",
    "            new_used.append(('{}_encoding_{}'.format(col, key), key))\n",
    "\n",
    "    for func in ['mean', 'std', 'median']: #19.09.22 \n",
    "        for col in new_used:\n",
    "            data[\"_\".join([col[0], func])] = df.groupby(col[1])[col[0]].transform(func)\n",
    "#     for key in ['c14_d1_uid']:\n",
    "#         for col in ['TransactionAmt', 'TransactionDT']:\n",
    "#             data['{}_diff_1_{}'.format(col, key)] = \\\n",
    "#             df[col] - df.groupby(key)[col].shift(1)\n",
    "#             data['{}_diff_-1_{}'.format(col, key)] = \\\n",
    "#             df[col] - df.groupby(key)[col].shift(-1)\n",
    "    return data\n",
    "\n",
    "if new_cols2:\n",
    "    train_transaction.drop(new_cols2, axis = 1, inplace = True)\n",
    "    test_transaction.drop(new_cols2, axis = 1, inplace = True)\n",
    "print(train_transaction.shape, test_transaction.shape)\n",
    "tr_shape = train_transaction.shape[0]\n",
    "new_feat = amt_accumulation_summary(train_transaction, test_transaction)\n",
    "train_transaction[new_feat.columns] = new_feat[:tr_shape].reset_index(drop = True)\n",
    "test_transaction[new_feat.columns] = new_feat[tr_shape:].reset_index(drop = True)\n",
    "new_cols2 = list(new_feat.columns)\n",
    "print(train_transaction.shape, test_transaction.shape)\n",
    "del new_feat;gc.collect()\n",
    "print(\"over.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# a = train_transaction.groupby(card_feature + addr_feature)['isFraud'].expanding(5).sum().reset_index()\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def expanding_helper(df, key, col, windows, func):\n",
    "    g = pd.DataFrame([])\n",
    "    name = \"_\".join([key, str(col), func, str(windows)])\n",
    "    if func == 'sum':\n",
    "        g[name] = df[col].apply(lambda x: x.expanding(windows).sum())\n",
    "    if func == 'mean':\n",
    "        g[name] = df[col].apply(lambda x: x.expanding(windows).mean())\n",
    "    if (func == 'future_mean') & (col != 'isFraud'):\n",
    "        cum_mean = df[col].apply(lambda x: x.expanding().mean())\n",
    "        mean = df[col].transform('mean')\n",
    "        g[name] = cum_mean - mean\n",
    "    return g[name]\n",
    "\n",
    "def expanding_feature(tr, ts):\n",
    "    feat = ['TransactionID',\"P_emaildomain\", 'TransactionDT', 'TransactionAmt'] +\\\n",
    "            d_feature + card_feature + addr_feature + c_feature\n",
    "    df = pd.concat([tr[feat + ['isFraud']], ts[feat]]).reset_index(drop = True)\n",
    "    H_move = 0\n",
    "    df[\"day\"] = (df[\"TransactionDT\"] + 3600 * H_move) // (24 * 60 * 60)\n",
    "    df['D1_delta'] = df['D1'] - df['day']\n",
    "    df['cid'] = \\\n",
    "    df[c_feat + card_feature + \n",
    "       addr_feature + ['D1_delta']].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    df['uid'] = df[card_feature + addr_feature + ['P_emaildomain']].apply(\n",
    "                    lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    df['c4_d1_id'] = df[['C4', 'D1_delta'] + card_feature + addr_feature].apply(\n",
    "                    lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    data = pd.DataFrame([])\n",
    "    cols_expanding = ['isFraud']\n",
    "#     cols_expanding = ['TransactionAmt']\n",
    "    for key in ['c4_d1_id']:\n",
    "        if key == 'cid':\n",
    "            windows = [50, 100, 150, 200]\n",
    "        else:\n",
    "            windows = [12, 24, 36]\n",
    "        GroupBy = df.groupby([key])\n",
    "        res = Parallel(n_jobs = 12, backend = 'multiprocessing') \\\n",
    "            (delayed(expanding_helper)(GroupBy, key, col, period, func) \n",
    "             for col in cols_expanding\n",
    "             for period in windows\n",
    "             for func in ['mean']\n",
    "            )\n",
    "        ans = pd.concat(res, axis = 1)\n",
    "        data[ans.columns] = ans\n",
    "    #counting\n",
    "    #sum/mean ...\n",
    "#     key_list = ['uid', 'c4_d1_id', 'cid']\n",
    "#     for key in key_list:\n",
    "#         for col in ['TransactionAmt']:\n",
    "#             cumcount = df.groupby(key)[col].apply(lambda x: x.expanding().mean())\n",
    "#             count_ = df.groupby(key)[col].transform('mean')\n",
    "#             cname = \"_\".join([key, str(col)]) + \"_future_mean\"\n",
    "#             data[cname] = count_ - cumcount\n",
    "    return data\n",
    "\n",
    "print(train_transaction.shape, test_transaction.shape)\n",
    "tr_shape = train_transaction.shape[0]\n",
    "new_feat = expanding_feature(train_transaction, test_transaction)\n",
    "train_transaction[new_feat.columns] = new_feat[:tr_shape].reset_index(drop = True)\n",
    "test_transaction[new_feat.columns] = new_feat[tr_shape:].reset_index(drop = True)\n",
    "new_cols3 = list(new_feat.columns)\n",
    "print(train_transaction.shape, test_transaction.shape)\n",
    "del new_feat;gc.collect()\n",
    "print(\"over.\")\n",
    "\n",
    "# if new_cols4:\n",
    "#     train_transaction.drop(new_cols4, axis = 1, inplace = True)\n",
    "#     test_transaction.drop(new_cols4, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(587087, 774) (506691, 778)\n",
      "(587087, 782) (506691, 786)\n",
      "over.\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "def amt_accumulation_standard(tr, ts):\n",
    "    v_list = ['V47', 'V244', 'V246', 'V257']\n",
    "    key = card_feature + addr_feature + c_feature + d_feature + v_list\n",
    "    df = pd.concat([tr[key + ['Transaction_day', 'TransactionDT','TransactionAmt','P_emaildomain']], \n",
    "                    ts[key + ['Transaction_day', 'TransactionDT','TransactionAmt','P_emaildomain']]]).reset_index(drop = True)\n",
    "    H_move = 0\n",
    "    df[\"Transaction_day\"] = (df[\"TransactionDT\"] + 3600 * H_move) // (24 * 60 * 60)\n",
    "    col = 'D1'\n",
    "    df[col + '_new'] = df[col] - df['Transaction_day']\n",
    "    col = 'D10'\n",
    "    df[col + '_new'] = df[col] - df['Transaction_day']\n",
    "    col = 'D15'\n",
    "    df[col + '_new'] = df[col] - df['Transaction_day']\n",
    "    \n",
    "    data = pd.DataFrame([])\n",
    "    df['uid'] = df[card_feature + addr_feature + [\"P_emaildomain\"]].apply(\n",
    "                    lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    df['d1_uid'] = df['uid'].astype(str) + \"_\" + df['D1_new'].astype(str)\n",
    "    \n",
    "    df['c4_d1_id'] = \\\n",
    "    df[['C4', 'D1_new'] + card_feature].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    df['d10_d1_uid'] = \\\n",
    "    df[['D10_new', 'D1_new']+ card_feature].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    df['d15_d1_uid'] = \\\n",
    "    df[['D15_new', 'D1_new']+ card_feature].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    df['most_accurate_id'] = \\\n",
    "    df[card_feature + addr_feature + [\"P_emaildomain\"] + c_feat + ['D1_new', 'D10_new']].apply(\n",
    "                    lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "            \n",
    "    new_used = []\n",
    "    for key in ['uid', 'd1_uid', 'c4_d1_id'] + ['most_accurate_id']:\n",
    "#          + ['C14', 'C1']\n",
    "        for col in ['TransactionAmt']:\n",
    "            df['{}_standard_{}'.format(col, key)] = df[col]\n",
    "            new_used.append(('{}_standard_{}'.format(col, key), key))\n",
    "            \n",
    "    for func in ['mean', 'median']: #19.09.22 \n",
    "        for col in new_used:\n",
    "            data[\"_\".join([col[0], func])] = df[col[0]] - df.groupby(col[1])[col[0]].transform(func)\n",
    "#     for key in ['c14_d1_uid']:\n",
    "#         for col in ['TransactionAmt', 'TransactionDT']:\n",
    "#             data['{}_diff_1_{}'.format(col, key)] = \\\n",
    "#             df[col] - df.groupby(key)[col].shift(1)\n",
    "#             data['{}_diff_-1_{}'.format(col, key)] = \\\n",
    "#             df[col] - df.groupby(key)[col].shift(-1)\n",
    "    return data\n",
    "\n",
    "if new_cols4:\n",
    "    train_transaction.drop(new_cols4, axis = 1, inplace = True)\n",
    "    test_transaction.drop(new_cols4, axis = 1, inplace = True)\n",
    "print(train_transaction.shape, test_transaction.shape)\n",
    "tr_shape = train_transaction.shape[0]\n",
    "new_feat = amt_accumulation_standard(train_transaction, test_transaction)\n",
    "train_transaction[new_feat.columns] = new_feat[:tr_shape].reset_index(drop = True)\n",
    "test_transaction[new_feat.columns] = new_feat[tr_shape:].reset_index(drop = True)\n",
    "new_cols4 = list(new_feat.columns)\n",
    "print(train_transaction.shape, test_transaction.shape)\n",
    "del new_feat;gc.collect()\n",
    "print(\"over.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cp ieee_moelling_with_rule_v5.ipynb /ieee/pipeline/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "727\n",
      "train.shape = (587087, 782), test.shape = (506691, 786)\n",
      "############################################################ fold = 1 / 5\n",
      "####### cur time = 2019/09/23 09:48:11\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid_0's auc: 0.93203\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-03cfd71b96c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbasic_feature\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnew_cols\u001b[0m \u001b[0;34m+\u001b[0m        \u001b[0mnew_cols1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnew_cols2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnew_cols3\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnew_cols4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md_feature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mused\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0moof_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moof_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfold_lightgbm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_transaction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_transaction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/root/ieee/pipeline/model.py\u001b[0m in \u001b[0;36mkfold_lightgbm\u001b[0;34m(tr, ts, features, params_new, n_kfolds)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mnum_boost_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mearly_stopping_rounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0mverbose_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         )\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    202\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1526\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1527\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1528\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1529\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from model import kfold_lightgbm\n",
    "used = [col for col in features + basic_feature + new_cols +\\\n",
    "        new_cols1 + new_cols2 + new_cols3 + new_cols4 if col not in d_feature]\n",
    "print(len(used))\n",
    "oof_train, oof_test, score_list = kfold_lightgbm(train_transaction, test_transaction, used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "725\n",
      "train.shape = (587087, 780), test.shape = (506691, 784)\n",
      "############################################################ fold = 1 / 5\n",
      "####### cur time = 2019/09/22 21:59:34\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid_0's auc: 0.933458\n",
      "[1000]\tvalid_0's auc: 0.9414\n",
      "[1500]\tvalid_0's auc: 0.941969\n",
      "Early stopping, best iteration is:\n",
      "[1431]\tvalid_0's auc: 0.942064\n",
      "period: [     0      1      2 ... 117415 117416 117417] , the score is 0.9420644452655844\n",
      "############################################################ fold = 2 / 5\n",
      "####### cur time = 2019/09/22 22:08:57\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid_0's auc: 0.959711\n",
      "[1000]\tvalid_0's auc: 0.963384\n",
      "Early stopping, best iteration is:\n",
      "[1253]\tvalid_0's auc: 0.963631\n",
      "period: [117418 117419 117420 ... 234833 234834 234835] , the score is 0.963631486634543\n",
      "############################################################ fold = 3 / 5\n",
      "####### cur time = 2019/09/22 22:17:00\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid_0's auc: 0.958909\n",
      "[1000]\tvalid_0's auc: 0.962713\n",
      "Early stopping, best iteration is:\n",
      "[1245]\tvalid_0's auc: 0.962948\n",
      "period: [234836 234837 234838 ... 352250 352251 352252] , the score is 0.9629477415397738\n",
      "############################################################ fold = 4 / 5\n",
      "####### cur time = 2019/09/22 22:25:05\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid_0's auc: 0.966059\n",
      "[1000]\tvalid_0's auc: 0.968633\n",
      "Early stopping, best iteration is:\n",
      "[1245]\tvalid_0's auc: 0.968778\n",
      "period: [352253 352254 352255 ... 469667 469668 469669] , the score is 0.9687775985626643\n",
      "############################################################ fold = 5 / 5\n",
      "####### cur time = 2019/09/22 22:33:10\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid_0's auc: 0.949104\n",
      "[1000]\tvalid_0's auc: 0.952547\n",
      "[1500]\tvalid_0's auc: 0.953231\n",
      "Early stopping, best iteration is:\n",
      "[1554]\tvalid_0's auc: 0.953266\n",
      "period: [469670 469671 469672 ... 587084 587085 587086] , the score is 0.9532660349846722\n",
      "score_list_mean = 0.958137\n",
      "score full train = 0.959289\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from model import kfold_lightgbm\n",
    "used = [col for col in features + basic_feature + new_cols +\\\n",
    "        new_cols1 + new_cols2 + new_cols3 + new_cols4 if col not in d_feature]\n",
    "print(len(used))\n",
    "oof_train, oof_test, score_list = kfold_lightgbm(train_transaction, test_transaction, used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719\n",
      "train.shape = (587087, 774), test.shape = (506691, 778)\n",
      "############################################################ fold = 1 / 5\n",
      "####### cur time = 2019/09/22 19:21:28\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid_0's auc: 0.933254\n",
      "[1000]\tvalid_0's auc: 0.941416\n",
      "Early stopping, best iteration is:\n",
      "[1339]\tvalid_0's auc: 0.942209\n",
      "period: [     0      1      2 ... 117415 117416 117417] , the score is 0.9422092898315451\n",
      "############################################################ fold = 2 / 5\n",
      "####### cur time = 2019/09/22 19:30:08\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid_0's auc: 0.959661\n",
      "[1000]\tvalid_0's auc: 0.963228\n",
      "Early stopping, best iteration is:\n",
      "[1332]\tvalid_0's auc: 0.963522\n",
      "period: [117418 117419 117420 ... 234833 234834 234835] , the score is 0.963521961364452\n",
      "############################################################ fold = 3 / 5\n",
      "####### cur time = 2019/09/22 19:38:21\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid_0's auc: 0.958821\n",
      "[1000]\tvalid_0's auc: 0.962335\n",
      "Early stopping, best iteration is:\n",
      "[1344]\tvalid_0's auc: 0.962781\n",
      "period: [234836 234837 234838 ... 352250 352251 352252] , the score is 0.9627812899756787\n",
      "############################################################ fold = 4 / 5\n",
      "####### cur time = 2019/09/22 19:46:42\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid_0's auc: 0.966152\n",
      "[1000]\tvalid_0's auc: 0.968703\n",
      "Early stopping, best iteration is:\n",
      "[1172]\tvalid_0's auc: 0.96884\n",
      "period: [352253 352254 352255 ... 469667 469668 469669] , the score is 0.9688403094946167\n",
      "############################################################ fold = 5 / 5\n",
      "####### cur time = 2019/09/22 19:54:08\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid_0's auc: 0.949208\n",
      "[1000]\tvalid_0's auc: 0.952353\n",
      "[1500]\tvalid_0's auc: 0.952949\n",
      "Early stopping, best iteration is:\n",
      "[1436]\tvalid_0's auc: 0.952995\n",
      "period: [469670 469671 469672 ... 587084 587085 587086] , the score is 0.9529952009606291\n",
      "score_list_mean = 0.958070\n",
      "score full train = 0.958961\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from model import kfold_lightgbm\n",
    "used = [col for col in features + basic_feature + new_cols +\\\n",
    "        new_cols1 + new_cols2 + new_cols3 if col not in d_feature]\n",
    "print(len(used))\n",
    "oof_train, oof_test, score_list = kfold_lightgbm(train_transaction, test_transaction, used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\tvalid_0's auc: 0.932762\n",
      "Early stopping, best iteration is:\n",
      "[1239]\tvalid_0's auc: 0.968019\n",
      "period: [352253 352254 352255 ... 469667 469668 469669] , the score is 0.9680192595089138\n",
      "############################################################ fold = 5 / 5\n",
      "####### cur time = 2019/09/22 10:06:07\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid_0's auc: 0.949395\n",
      "[1000]\tvalid_0's auc: 0.952914\n",
      "[1500]\tvalid_0's auc: 0.95359\n",
      "Early stopping, best iteration is:\n",
      "[1603]\tvalid_0's auc: 0.95365\n",
      "period: [469670 469671 469672 ... 587084 587085 587086] , the score is 0.9536504764202409\n",
      "score_list_mean = 0.957900\n",
      "score full train = 0.957981\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from model import kfold_lightgbm\n",
    "used = [col for col in features + basic_feature + new_cols +\\\n",
    "        new_cols1 + new_cols2 + new_cols3 if col not in d_feature]\n",
    "print(len(used))\n",
    "oof_train, oof_test, score_list = kfold_lightgbm(train_transaction, test_transaction, used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin.\n",
      "fraud num: 3641\n",
      "begin.\n",
      "fraud num: 2111\n",
      "begin.\n",
      "fraud num: 171\n",
      "begin.\n",
      "fraud num: 34369\n",
      "CPU times: user 5.96 s, sys: 15 s, total: 21 s\n",
      "Wall time: 6min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from post import online_rule_parallel\n",
    "col = 'D1'\n",
    "train_transaction[col + '_new'] = train_transaction[col] - train_transaction['Transaction_day']\n",
    "test_transaction[col + '_new'] = test_transaction[col] - test_transaction['Transaction_day']\n",
    "for col in d_feature:\n",
    "    train_transaction[col + '_new'] = train_transaction[col] - train_transaction['Transaction_day']\n",
    "    test_transaction[col + '_new'] = test_transaction[col] - test_transaction['Transaction_day']\n",
    "\n",
    "\n",
    "count_list = [[col] + card_feature + ['D1_new'] for col in ['C4', 'C5']]\n",
    "basis_list = [card_feature + addr_feature + c_feat + ['D1_new'],\n",
    "              card_feature + addr_feature + ['D1_new']\n",
    "             ]\n",
    "day_list =  [[col] + card_feature + ['D1_new'] for col in ['D10_new', 'D15_new']] +\\\n",
    "            [[col] + card_feature + addr_feature for col in ['D2_new', 'D4_new']] +\\\n",
    "            [[col] + card_feature + c_feat for col in ['D4_new', 'D10_new']]\n",
    "d_new_feature = [\"{}_new\".format(col) for col in d_feature if col not in ['D1']]\n",
    "\n",
    "\n",
    "full_list = count_list + basis_list + day_list\n",
    "\n",
    "feat_rule = c_feat + ['D1_new', 'TransactionID', 'Transaction_day'] + card_feature + addr_feature + d_new_feature\n",
    "tr_rule = train_transaction[feat_rule + ['isFraud']]\n",
    "ts_rule = test_transaction[feat_rule]\n",
    "test_fraud = online_rule_parallel(tr_rule, ts_rule, full_list)\n",
    "test_fraud.drop_duplicates(['TransactionID'], inplace = True)\n",
    "# fraud num: 2749\n",
    "# CPU times: user 3.77 s, sys: 8.86 s, total: 12.6 s\n",
    "# Wall time: 2min 31s\n",
    "print(\"fraud num:\", test_fraud.shape[0])\n",
    "del tr_rule,ts_rule;gc.collect()\n",
    "from post import online_special_black_parallel\n",
    "for col in d_feature:\n",
    "    train_transaction[col + '_new'] = train_transaction[col] - train_transaction['Transaction_day']\n",
    "    test_transaction[col + '_new'] = test_transaction[col] - test_transaction['Transaction_day']\n",
    "\n",
    "d_new_feature = [\"{}_new\".format(col) for col in d_feature if col not in ['D1']]\n",
    "feat_rule = c_feat + ['D1_new', 'TransactionID', 'Transaction_day'] + card_feature + addr_feature + d_new_feature\n",
    "tr_rule = train_transaction[feat_rule + ['isFraud']]\n",
    "ts_rule = test_transaction[feat_rule]\n",
    "\n",
    "\n",
    "count_list = [[col] + card_feature + ['D1_new'] for col in ['C12']]\n",
    "day_list =  [[col] + card_feature + ['D1_new'] for col in ['D15_new']] +\\\n",
    "            [[col] + card_feature + addr_feature for col in ['D2_new']]\n",
    "        \n",
    "full_list = count_list + day_list\n",
    "\n",
    "special_fraud = online_special_black_parallel(tr_rule, ts_rule, full_list)\n",
    "special_fraud.drop_duplicates(['TransactionID'], inplace = True)\n",
    "print(\"fraud num:\", special_fraud.shape[0])\n",
    "# fraud num: 3683\n",
    "# CPU times: user 3.57 s, sys: 1.75 s, total: 5.32 s\n",
    "# Wall time: 6min 34s\n",
    "del tr_rule,ts_rule;gc.collect()\n",
    "from post import online_grey_parallel\n",
    "for col in d_feature:\n",
    "    train_transaction[col + '_new'] = train_transaction[col] - train_transaction['Transaction_day']\n",
    "    test_transaction[col + '_new'] = test_transaction[col] - test_transaction['Transaction_day']\n",
    "\n",
    "d_new_feature = [\"{}_new\".format(col) for col in d_feature if col not in ['D1']]\n",
    "day_list =  [[col] + card_feature + ['D1_new'] for col in ['D11_new']]\n",
    "#plus D11_new out of research.\n",
    "full_list = day_list\n",
    "\n",
    "feat_rule = c_feat + ['D1_new', 'TransactionID', 'Transaction_day'] + card_feature + addr_feature + d_new_feature\n",
    "tr_rule = train_transaction[feat_rule + ['isFraud']]\n",
    "ts_rule = test_transaction[feat_rule]\n",
    "test_grey = online_grey_parallel(tr_rule, ts_rule, full_list)\n",
    "test_grey.drop_duplicates(['TransactionID'], inplace = True)\n",
    "print(\"fraud num:\", test_grey.shape[0])\n",
    "# fraud num: 1279\n",
    "# CPU times: user 3.95 s, sys: 2.12 s, total: 6.06 s\n",
    "# Wall time: 6min 59s\n",
    "del tr_rule,ts_rule;gc.collect()\n",
    "from post import online_white_parallel\n",
    "tr_rule = train_transaction[feat_rule + ['isFraud']]\n",
    "ts_rule = test_transaction[feat_rule]\n",
    "full_list = [card_feature + addr_feature + c_feat + ['D1_new'], card_feature + c_feat + ['D1_new']]\n",
    "\n",
    "test_white = online_white_parallel(train_transaction, test_transaction, full_list)\n",
    "test_white.drop_duplicates(['TransactionID'], inplace = True)\n",
    "# fraud num: 34369\n",
    "# CPU times: user 2.47 s, sys: 4.66 s, total: 7.13 s\n",
    "# Wall time: 2min 41s\n",
    "print(\"fraud num:\", test_white.shape[0])\n",
    "del tr_rule,ts_rule;gc.collect()\n",
    "\n",
    "\n",
    "# begin.\n",
    "# fraud num: 2749\n",
    "# begin.\n",
    "# fraud num: 2039\n",
    "# begin.\n",
    "# fraud num: 837\n",
    "# begin.\n",
    "# fraud num: 34369\n",
    "# CPU times: user 10.3 s, sys: 27.2 s, total: 37.5 s\n",
    "# Wall time: 8min 41s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin.\n",
      "CPU times: user 47.8 s, sys: 1min 28s, total: 2min 16s\n",
      "Wall time: 2min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"begin.\")\n",
    "if 'fraud_guess' in test_transaction.columns:\n",
    "    test_transaction.drop(['fraud_guess'], axis = 1, inplace = True)\n",
    "test_transaction['pred'] = oof_test.mean(axis = 1)\n",
    "test_transaction = pd.merge(test_transaction, test_fraud, on = ['TransactionID'], how = 'left')\n",
    "test_transaction.fraud_guess.fillna(0, inplace = True)\n",
    "test_transaction['pred'] = np.where(test_transaction['fraud_guess'] == 1, 1, test_transaction['pred'])\n",
    "\n",
    "if 'special_fraud_guess' in test_transaction.columns:\n",
    "    test_transaction.drop(['special_fraud_guess'], axis = 1, inplace = True)\n",
    "test_transaction = pd.merge(test_transaction, special_fraud, on = ['TransactionID'], how = 'left')\n",
    "test_transaction.special_fraud_guess.fillna(0, inplace = True)\n",
    "test_transaction['pred'] = np.where(test_transaction['special_fraud_guess'] == 1, 1, test_transaction['pred'])\n",
    "\n",
    "\n",
    "if 'grey_guess' in test_transaction.columns:\n",
    "    test_transaction.drop(['grey_guess'], axis = 1, inplace = True)\n",
    "test_transaction = pd.merge(test_transaction, test_grey, on = ['TransactionID'], how = 'left')\n",
    "test_transaction.grey_guess.fillna(0, inplace = True)\n",
    "test_transaction['pred'] = np.where(test_transaction['grey_guess'] == 1, 1, test_transaction['pred'])\n",
    "\n",
    "if 'white_guess' in test_transaction.columns:\n",
    "    test_transaction.drop(['white_guess'], axis = 1, inplace = True)\n",
    "test_transaction = pd.merge(test_transaction, test_white, on = ['TransactionID'], how = 'left')\n",
    "test_transaction.white_guess.fillna(0, inplace = True)\n",
    "test_transaction['pred'] = np.where(test_transaction['white_guess'] == 1, 0, test_transaction['pred'])\n",
    "\n",
    "\n",
    "from time import localtime, strftime\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# score = roc_auc_score(train_transaction['isFraud'],oof_train)\n",
    "score = roc_auc_score(train_transaction['isFraud'],oof_train)\n",
    "mean_score = np.mean(score_list[0:5])\n",
    "sub = pd.DataFrame([])\n",
    "strategy = 'KFold'\n",
    "sub['TransactionID'] = test_transaction['TransactionID']\n",
    "sub['isFraud'] = test_transaction['pred']\n",
    "black_num = \"black:{}\".format(test_transaction.loc[(test_transaction['fraud_guess'] == 1)].shape[0])\n",
    "special_num = \"special:{}\".format(test_transaction.loc[(test_transaction['special_fraud_guess'] == 1)].shape[0])\n",
    "white_num = \"white:{}\".format(test_transaction.loc[(test_transaction['white_guess'] == 1)].shape[0])\n",
    "grey_num = \"grey:{}\".format(test_transaction.loc[(test_transaction['grey_guess'] == 1)].shape[0])\n",
    "sub.to_csv('/root/ieee/sub/{}_{}_oof_{}_mean_{}_post_{}_{}_{}_{}.csv'.format(strategy,\n",
    "                                                    strftime(\"%Y_%m%d_%H%M\", localtime()), \n",
    "                                                    np.around(score,5), np.around(mean_score, 5),\n",
    "                                                    black_num, special_num, white_num, grey_num), index = None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def null_importance(train_df, train_features, nb_runs = 20):\n",
    "    def get_feature_importances(data, train_features, shuffle = False):\n",
    "        y = data['isFraud'].copy()\n",
    "        if shuffle:\n",
    "        # Here you could as well use a binomial distribution\n",
    "            y = data['isFraud'].copy().sample(frac=1.0)\n",
    "        dtrain = lgb.Dataset(data[train_features], y, free_raw_data=False, silent=False)\n",
    "        params = {'num_leaves': 450,\n",
    "          'min_child_weight': 0.03,\n",
    "          'feature_fraction': 0.37,\n",
    "          'bagging_fraction': 0.42,\n",
    "          'min_data_in_leaf': 106,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.01,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'auc',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.39,\n",
    "          'reg_lambda': 0.65,\n",
    "          'random_state': 47,\n",
    "          # 'seed':0\n",
    "         }\n",
    "        clf = lgb.train(params=params,train_set=dtrain,num_boost_round=100)\n",
    "        preds = clf.predict(data[train_features])\n",
    "        # Get feature importances\n",
    "        imp_df = pd.DataFrame()\n",
    "        imp_df[\"feature\"] = list(train_features)\n",
    "        imp_df[\"importance_gain\"] = clf.feature_importance(importance_type='gain')\n",
    "        imp_df[\"importance_split\"] = clf.feature_importance(importance_type='split')\n",
    "        imp_df['trn_score'] = roc_auc_score(y, preds)\n",
    "        del dtrain;gc.collect()\n",
    "        return imp_df\n",
    "    np.random.seed(817)\n",
    "    # Get the actual importance, i.e. without shuffling\n",
    "    actual_imp_df = get_feature_importances(data = train_df, train_features = train_features, shuffle=False)\n",
    "    null_imp_df = pd.DataFrame()\n",
    "    start = time.time()\n",
    "    dsp = ''\n",
    "    for i in range(nb_runs):\n",
    "        print('round:', i)\n",
    "    # Get current run importances\n",
    "        imp_df = get_feature_importances(data=train_df, train_features = train_features, shuffle=True)\n",
    "        imp_df['run'] = i + 1\n",
    "    # Concat the latest importances with the old ones\n",
    "        null_imp_df = pd.concat([null_imp_df, imp_df], axis=0)\n",
    "    # Erase previous message\n",
    "        for l in range(len(dsp)):\n",
    "            print('\\b', end='', flush=True)\n",
    "    # Display current run and time used\n",
    "    spent = (time.time() - start) / 60\n",
    "    dsp = 'Done with %4d of %4d (Spent %5.1f min)' % (i + 1, nb_runs, spent)\n",
    "    print(dsp, end='', flush=True)\n",
    "    feature_scores = []\n",
    "    for _f in actual_imp_df['feature'].unique():\n",
    "        f_null_imps_gain = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values\n",
    "        f_act_imps_gain = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].mean()\n",
    "\n",
    "        gain_score = np.log(1e-10 + f_act_imps_gain / (1 + np.percentile(f_null_imps_gain, 75)))  # Avoid didvide by zero\n",
    "        f_null_imps_split = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values\n",
    "        f_act_imps_split = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].mean()\n",
    "        split_score = np.log(1e-10 + f_act_imps_split / (1 + np.percentile(f_null_imps_split, 75)))  # Avoid didvide by zero\n",
    "        feature_scores.append((_f, split_score, gain_score))\n",
    "\n",
    "    scores_df = pd.DataFrame(feature_scores, columns=['feature', 'split_score', 'gain_score'])\n",
    "    new_list = scores_df.sort_values(by=['gain_score'],ascending=False).reset_index(drop=True)\n",
    "    return new_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round: 0\n",
      "round: 1\n",
      "round: 2\n",
      "round: 3\n",
      "round: 4\n",
      "round: 5\n",
      "round: 6\n",
      "round: 7\n",
      "round: 8\n",
      "round: 9\n",
      "round: 10\n",
      "round: 11\n",
      "round: 12\n",
      "round: 13\n",
      "round: 14\n",
      "round: 15\n",
      "round: 16\n",
      "round: 17\n",
      "round: 18\n",
      "round: 19\n",
      "round: 20\n",
      "round: 21\n",
      "round: 22\n",
      "round: 23\n",
      "round: 24\n",
      "round: 25\n",
      "round: 26\n",
      "round: 27\n",
      "round: 28\n",
      "round: 29\n",
      "round: 30\n",
      "round: 31\n",
      "round: 32\n",
      "round: 33\n",
      "round: 34\n",
      "round: 35\n",
      "round: 36\n",
      "round: 37\n",
      "round: 38\n",
      "round: 39\n",
      "round: 40\n",
      "round: 41\n",
      "round: 42\n",
      "round: 43\n",
      "round: 44\n",
      "round: 45\n",
      "round: 46\n",
      "round: 47\n",
      "round: 48\n",
      "round: 49\n",
      "Done with   50 of   50 (Spent  53.6 min)511\n",
      "train.shape = (587087, 774), test.shape = (506691, 778)\n",
      "############################################################ fold = 1 / 5\n",
      "####### cur time = 2019/09/22 21:16:25\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid_0's auc: 0.933044\n",
      "[1000]\tvalid_0's auc: 0.940294\n",
      "Early stopping, best iteration is:\n",
      "[1267]\tvalid_0's auc: 0.94077\n",
      "period: [     0      1      2 ... 117415 117416 117417] , the score is 0.940769735220405\n",
      "############################################################ fold = 2 / 5\n",
      "####### cur time = 2019/09/22 21:22:50\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid_0's auc: 0.960038\n",
      "[1000]\tvalid_0's auc: 0.963445\n",
      "Early stopping, best iteration is:\n",
      "[1037]\tvalid_0's auc: 0.963471\n",
      "period: [117418 117419 117420 ... 234833 234834 234835] , the score is 0.9634712918177861\n",
      "############################################################ fold = 3 / 5\n",
      "####### cur time = 2019/09/22 21:28:01\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid_0's auc: 0.959336\n",
      "[1000]\tvalid_0's auc: 0.962848\n",
      "Early stopping, best iteration is:\n",
      "[1285]\tvalid_0's auc: 0.963298\n",
      "period: [234836 234837 234838 ... 352250 352251 352252] , the score is 0.9632983191102594\n",
      "############################################################ fold = 4 / 5\n",
      "####### cur time = 2019/09/22 21:34:13\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid_0's auc: 0.966342\n",
      "[1000]\tvalid_0's auc: 0.968923\n",
      "Early stopping, best iteration is:\n",
      "[1010]\tvalid_0's auc: 0.968946\n",
      "period: [352253 352254 352255 ... 469667 469668 469669] , the score is 0.968946482991073\n",
      "############################################################ fold = 5 / 5\n",
      "####### cur time = 2019/09/22 21:39:18\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid_0's auc: 0.949659\n",
      "[1000]\tvalid_0's auc: 0.952771\n",
      "[1500]\tvalid_0's auc: 0.953192\n",
      "Early stopping, best iteration is:\n",
      "[1526]\tvalid_0's auc: 0.953237\n",
      "period: [469670 469671 469672 ... 587084 587085 587086] , the score is 0.9532371338369356\n",
      "score_list_mean = 0.957945\n",
      "score full train = 0.958515\n"
     ]
    }
   ],
   "source": [
    "from model import kfold_lightgbm\n",
    "var_result = null_importance(train_transaction, used, 50)\n",
    "null_feat = list(var_result.loc[var_result['gain_score'] >= 0, 'feature'])\n",
    "print(len(null_feat))\n",
    "#669?\n",
    "oof_train, oof_test, score_list = kfold_lightgbm(train_transaction, test_transaction, null_feat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
